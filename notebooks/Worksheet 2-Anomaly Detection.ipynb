{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/GTK_Logo_Social Icon.jpg\" width=175 align=\"right\" />\n",
    "\n",
    "\n",
    "# Worksheet 2.0 Anomaly Detection\n",
    "\n",
    "This worksheet covers concepts relating to Anomaly Detection.  It should take no more than 20-30 minutes to complete.  Please raise your hand if you get stuck.  \n",
    "\n",
    "There are many ways to accomplish the tasks that you are presented with, however you will find that by using the techniques covered in class, the exercises should be relatively simple. \n",
    "\n",
    "## Import the Libraries\n",
    "For this exercise, we will be using:\n",
    "* Pandas (http://pandas.pydata.org/pandas-docs/stable/)\n",
    "* Numpy (https://docs.scipy.org/doc/numpy/reference/)\n",
    "* Matplotlib (http://matplotlib.org/api/pyplot_api.html)\n",
    "* StatsModels (https://www.statsmodels.org/stable/index.html)\n",
    "* Pmdarima (http://alkaline-ml.com/pmdarima/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-27T19:19:09.257956Z",
     "start_time": "2023-03-27T19:19:09.250266Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pmdarima.arima import auto_arima\n",
    "from pmdarima.arima import ADFTest\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import plot_predict\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "style.use(\"ggplot\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part One:  Finding Anomalies in CPU Usage Data\n",
    "The first part of this lab, you will be examining CPU usage data to find anomalies. \n",
    "\n",
    "## Step One:  Get the Data\n",
    "For this example, we will be looking at CPU Utilization Data to see if we can identify periods of unusual activity.  The data can be found in several files:\n",
    "\n",
    "* `cpu-full-a.csv`:  A full set of CPU usage data without anomalies\n",
    "* `cpu-train-a.csv`:  The training set from data set A\n",
    "* `cpu-test-b.csv`:  The test set from data set A\n",
    "* `cpu-full-b.csv`:  A full set of CPU usage data with an anomaly\n",
    "* `cpu-train-b.csv`:  The training set from data set A\n",
    "* `cpu-test-b.csv`:  The test set from data set A\n",
    "\n",
    "\n",
    "This dataset is from examples in *Machine Learning & Security*  by Clarence Chio and David Freeman.  https://github.com/oreilly-mlsec/book-resources/tree/master/chapter3/datasets/cpu-utilization.\n",
    "\n",
    "First let's take a look at the data set A.  For the first part of this lab, load the training dataset into a dataframe.  DataFrames have an option `infer_datatime_format` which, when set to `True`, will automatically infer dates. Setting this will save time and steps in data preparation. \n",
    "\n",
    "Once the data is loaded, call the usual series of exploratory functions and most importantly, plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-27T17:04:15.685681Z",
     "start_time": "2023-03-27T17:04:15.671935Z"
    }
   },
   "outputs": [],
   "source": [
    "df = # Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2:  Is the Data Stationary?\n",
    "\n",
    "Now, we are going to check to the stationarity of our data set.  Stationarity is a measurement of whether the data has seasonal trends or not.[1]\n",
    "\n",
    "First compute the rolling mean and standard deviation for the CPU column in the data set.  This can be accomplished with the `rolling` function.[2]  Try different window sizes. \n",
    "\n",
    "[1]: https://towardsdatascience.com/stationarity-in-time-series-analysis-90c94f27322\n",
    "[2]: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rolling.html\n",
    "\n",
    "Once you have computed the rolling mean and std, plot them on a graph with the original data.  If the lines are generally flat, we know that the data is stationary. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-27T17:23:59.042553Z",
     "start_time": "2023-03-27T17:23:59.038772Z"
    }
   },
   "outputs": [],
   "source": [
    "rolling_mean = # Your code here...\n",
    "rolling_std = # Your code here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-27T17:23:59.739974Z",
     "start_time": "2023-03-27T17:23:59.577406Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are going to run a test called the Dickey-Fuller [1] test on this data to prove whether the data is stationary or not.  Use the `adfuller` method in statsmodels to perform this computation. (https://www.statsmodels.org/devel/generated/statsmodels.tsa.stattools.adfuller.html)  For this example, use `AIC` as the autolag parameter which means that the lag is chosen to minimize the information criterion.\n",
    "\n",
    "[1]: https://en.wikipedia.org/wiki/Dickeyâ€“Fuller_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-27T17:32:47.554382Z",
     "start_time": "2023-03-27T17:32:47.540562Z"
    }
   },
   "outputs": [],
   "source": [
    "adft = # Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will present the results in a more understandable manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-27T17:18:17.008777Z",
     "start_time": "2023-03-27T17:18:16.998853Z"
    }
   },
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame({\"Values\":[adft[0],adft[1],adft[2],adft[3], adft[4]['1%'], adft[4]['5%'], adft[4]['10%']]  , \"Metric\":[\"Test Statistics\",\"p-value\",\"No. of lags used\",\"Number of observations used\", \n",
    "                                                        \"critical value (1%)\", \"critical value (5%)\", \"critical value (10%)\"]})\n",
    "output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So is the Data Stationary? \n",
    "If the `p-value` is greater than 5 and the test statistics are greater than the critical values, then we know that the data is not stationary.  What do you think?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Three:  Check for Autocorrelation\n",
    "The next step we want to determine is how correlated the time series is with past values. This will help us tune our model and also decide whether the data can be used at all.\n",
    "\n",
    "For this exercise, we will use the pandas `autocorrelation` methods.  \n",
    "\n",
    "#### References\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.Series.autocorr.html\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.plotting.autocorrelation_plot.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, calculate the autocorrelation at various lag intervals. This is calculating the Pearson correlation, so 1 indiciates perfect correlation.\n",
    "\n",
    "At what point does the correlation go below 75%?  50%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-27T17:45:56.116256Z",
     "start_time": "2023-03-27T17:45:56.104441Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create an autocorrelation plot using the pandas autocorrelation plot method. This plot will help us visualize whether the data is correlated with itself and what the lag periods are.\n",
    "\n",
    "(https://pandas.pydata.org/docs/reference/api/pandas.plotting.autocorrelation_plot.html) \n",
    "\n",
    "The method is: `pd.plotting.autocorrelation_plot(<data>)`.\n",
    "\n",
    "The horizontal lines in the plot correspond to 95% and 99% confidence bands.  The dashed line is 99% confidence band."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-27T17:59:13.774846Z",
     "start_time": "2023-03-27T17:59:13.575918Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Four:  Seasonal Decomposition\n",
    "The last analytic technique we're going to use here is seasonal decomposition. Using statsmodels `seasonal_decompose` create a decompose plot and let's take a look at the data.\n",
    "\n",
    "Use `additive` as the model type and try different values for the period. \n",
    "\n",
    "https://www.statsmodels.org/dev/generated/statsmodels.tsa.seasonal.seasonal_decompose.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-27T18:48:46.494140Z",
     "start_time": "2023-03-27T18:48:46.033667Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automation\n",
    "\n",
    "While is good to understand how this works, the module `pmdarima` actually has an automated test that can do this automatically. Try running the code below to determine whether the data is stationary or not.\n",
    "\n",
    "```python\n",
    "adf_test = ADFTest(alpha=0.05)\n",
    "adf_test.should_diff(df['cpu'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-27T19:19:51.761433Z",
     "start_time": "2023-03-27T19:19:51.740006Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Five:  Fit an ARIMA Model\n",
    "Since we are dealing with time series data, let's train an ARIMA model and see how well this technique fits the actual data. \n",
    "\n",
    "ARIMA has three parameters:\n",
    "\n",
    "* `p`:  The number of lag observations included in the model\n",
    "* `d`: The number of times the raw observations are differenced\n",
    "* `q`:  The size of the moving average window\n",
    "\n",
    "We are going to use the auto_arima method in pmdarima to do our forecasting.  Let's see how it works.  First build and fit an ARIMA model setting seasonal to `True`.  \n",
    "\n",
    "\n",
    "Docs:\n",
    "https://alkaline-ml.com/pmdarima/modules/generated/pmdarima.arima.auto_arima.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-27T19:43:12.727285Z",
     "start_time": "2023-03-27T19:43:07.435488Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run the `summary()` method to view some summary statistics for this model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-27T19:43:45.980352Z",
     "start_time": "2023-03-27T19:43:45.965636Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `predict_in_sample()` method, create a plot of the original data and the predictions to see how well the model did at forecasting with known data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-27T19:51:35.246804Z",
     "start_time": "2023-03-27T19:51:35.108890Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Three:  Find Anomalies in the CPU data\n",
    "Using data set `B` train a new model. Once you have a trained model, the next step is to call the `.predict()` method to generate 60 predictions.  \n",
    "\n",
    "Next, compare the predictions with the actual values in the test set, similar to how we assess the accuracy of a classifier.  We will call the difference between the actual and predicted value the anomaly score.  Calculate the anomaly score for the test data.  Finally, plot the anomaly scores, and see if you can find the time intervals with the highest anomaly score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-27T20:01:21.300144Z",
     "start_time": "2023-03-27T20:01:21.290663Z"
    }
   },
   "outputs": [],
   "source": [
    "df2_train = pd.read_csv('../data/cpu-train-b.csv', parse_dates=[0], infer_datetime_format=True)\n",
    "df2_test = pd.read_csv('../data/cpu-test-b.csv', parse_dates=[0], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-27T20:01:22.272363Z",
     "start_time": "2023-03-27T20:01:22.264121Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here...\n",
    "\n",
    "# Train a new model using the training dataset\n",
    "\n",
    "\n",
    "# Call predict and output 60 predictions and create a series with that\n",
    "\n",
    "\n",
    "# Create a series with the delta of the predictions and the test values.\n",
    "\n",
    "\n",
    "# Plot the resu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "If all went well, you should see anomalous behavior at 10 seconds into the test data.  Remembering that the forecasting's confidence goes down over time, the first anomaly should be enough to throw an alert for investigation. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
